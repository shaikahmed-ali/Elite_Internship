{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature Selection for Machine Learning\n\nIn the following document, I will delve into an exhaustive analysis of various feature selection techniques employed in the realm of machine learning. Gaining a profound comprehension of these methodologies holds a paramount significance, as it bears the potential to substantially diminish training durations and enhance the interpretability of machine learning models. Moreover, the judicious selection of pertinent features assumes a critical role in the transition of a model from development to production.\n\nFeature selection, a pivotal aspect of the machine learning pipeline, empowers practitioners to discern and retain the most pertinent attributes from a pool of available features. This discerning process isn't merely about reducing computational burden but also about enhancing model performance, generalization, and ultimately, facilitating its deployment in real-world scenarios.\n\nThe focus of the notebook is to explore diverse techniques, ranging from univariate statistical tests to advanced wrapper methods and embedded approaches, we aim to uncover the nuanced interplay between feature relevance and model efficacy. This exploration is characterized by an analytical scrutiny of feature importance, correlation, redundancy, and their collective influence on the model's behavior.\n\nThe significance of this undertaking extends beyond theoretical insights; it resonates practically when one considers the resource constraints often associated with real-world applications. Efficiently selecting features not only expedites training but also bolsters a model's ability to extrapolate patterns from new data instances.\n\nFurthermore, as machine learning models permeate various sectors, from finance to healthcare, the interpretability of these models is crucial. Understanding the rationale behind a model's decisions, which can be achieved through a well-considered feature selection process, not only builds trust but also aids in complying with regulatory standards.\n\n## Notebook aim \n\nThe primary objective of this notebook is to refrain from establishing a hierarchical ranking of feature selection techniques based on their effectiveness. This is because the efficacy of these techniques is contingent upon a myriad of factors, including the nature of the data, the specific business context, and the intended outcomes of the model. Instead, the notebook centers its focus on impartially visualizing and analyzing the diverse array of techniques, employing the data at hand without making any preemptive assumptions.\n\nIt's important to clarify that this notebook doesn't attempt to replicate a comprehensive data science pipeline. As a result, readers should not expect a step-by-step guide aimed at achieving the highest possible score in a competitive scenario. However, what this notebook does offer is invaluable guidance for individuals striving to attain such a favorable outcome.\n\nFor the sake of clarity, it's worth noting that this notebook deliberately avoids delving into the specifics of any particular data competition. Therefore, readers should not anticipate encountering discussions related to feature engineering or model comparisons. Instead, the focus remains squarely on the exploration of each individual feature selection technique. Each technique is executed with its default parameters, and the analysis remains independent of the nuances inherent to each specific technique.\n\nBy maintaining this approach, the notebook aspires to provide an unbiased and comprehensive overview of various feature selection strategies. This approach also safeguards against potential biases that could arise from tailored configurations geared toward achieving specific competition results. Thus, the notebook ultimately serves as a valuable resource for individuals seeking to gain a holistic understanding of these techniques, while avoiding the potential pitfalls of over-optimization.\n\n\n## The data\n\nThe Titanic dataset is a well-known and often-used dataset in the field of machine learning and data science. It is named after the infamous sinking of the RMS Titanic, a British passenger liner, during its maiden voyage in 1912. The dataset contains information about the passengers on board, including whether they survived the disaster or not. This dataset is commonly used for practicing and demonstrating various data analysis, feature engineering, and machine learning techniques.\n\nThe Titanic dataset is often used as a beginner-friendly introduction to data analysis and machine learning due to its relatively small size and historical significance. It provides opportunities to explore various aspects of data preprocessing, feature engineering, missing data imputation, visualization, and predictive modeling. The goal of machine learning tasks using this dataset is typically to predict whether a given passenger would have survived the disaster based on the available features.\n\nMany machine learning competitions and tutorials use the Titanic dataset as a starting point for learning different techniques and algorithms. It serves as a valuable resource for newcomers to the field to gain hands-on experience and insights into the process of data analysis and predictive modeling.\n\n### Kaggle files","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:53:53.349799Z","iopub.execute_input":"2023-08-20T18:53:53.350208Z","iopub.status.idle":"2023-08-20T18:53:53.379224Z","shell.execute_reply.started":"2023-08-20T18:53:53.350172Z","shell.execute_reply":"2023-08-20T18:53:53.378414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dependencies\n\nBefore executing the code in this notebook, it's crucial to install certain libraries that are not included by default in the Kaggle docker container. Make sure to install these libraries to ensure the smooth running of the code.","metadata":{}},{"cell_type":"code","source":"!pip install mlxtend","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:53:53.380693Z","iopub.execute_input":"2023-08-20T18:53:53.381441Z","iopub.status.idle":"2023-08-20T18:54:08.18272Z","shell.execute_reply.started":"2023-08-20T18:53:53.381411Z","shell.execute_reply":"2023-08-20T18:54:08.1818Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install feature-engine","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:08.183895Z","iopub.execute_input":"2023-08-20T18:54:08.184198Z","iopub.status.idle":"2023-08-20T18:54:21.879788Z","shell.execute_reply.started":"2023-08-20T18:54:08.18417Z","shell.execute_reply":"2023-08-20T18:54:21.87854Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install feature_engine","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:21.882343Z","iopub.execute_input":"2023-08-20T18:54:21.882668Z","iopub.status.idle":"2023-08-20T18:54:34.78702Z","shell.execute_reply.started":"2023-08-20T18:54:21.882639Z","shell.execute_reply":"2023-08-20T18:54:34.785638Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Principal Libraries","metadata":{}},{"cell_type":"code","source":"# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import set_config\n\n# Settings\npd.set_option('display.max_columns', None)\nsns.set_style('whitegrid')\nset_config(transform_output = \"pandas\")\n%matplotlib inline\n\n# Random State\nRANDOM_STATE = 16021822 # 16/02/1822, Sir Francis Galton!","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:34.788862Z","iopub.execute_input":"2023-08-20T18:54:34.789272Z","iopub.status.idle":"2023-08-20T18:54:35.738365Z","shell.execute_reply.started":"2023-08-20T18:54:34.789234Z","shell.execute_reply":"2023-08-20T18:54:35.73749Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Looking at the data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n# Dropping Cabin\ndata = data.drop(\"Cabin\", axis = 1)\n\n# Imputing Missing values\ndata[\"Age\"] = data[\"Age\"].fillna(data.Age.median())\ndata[\"Embarked\"] = data[\"Embarked\"].fillna(data.Embarked.mode().iloc[0])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:35.739714Z","iopub.execute_input":"2023-08-20T18:54:35.740013Z","iopub.status.idle":"2023-08-20T18:54:35.782938Z","shell.execute_reply.started":"2023-08-20T18:54:35.739987Z","shell.execute_reply":"2023-08-20T18:54:35.782024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X, y\nX = data.drop(\"Survived\", axis = 1)\ny = data[\"Survived\"]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:35.784068Z","iopub.execute_input":"2023-08-20T18:54:35.784364Z","iopub.status.idle":"2023-08-20T18:54:35.790728Z","shell.execute_reply.started":"2023-08-20T18:54:35.78434Z","shell.execute_reply":"2023-08-20T18:54:35.789644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Functions\n\nI will create a couple of functions to split, scaled, train certain model and then capture the metrics. Since I am gonna be testing many techniques this is gonna be crucial for efficiency and reduce code.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport time\n\n# Holding arrays\nTechnique = list()\nType = list()\nauc_score = list()\naccuracy = list()\nprecision = list()\nrecall = list()\nf1_Score = list()\ntraining_duration = list()\nalgorithm_duration = list()\nn_dropped = list()\nfeatures_kept = list()\nfeatures_dropped = list()\n\ndef split_data(X, y):\n    return train_test_split(X, y, test_size = 0.1, random_state = RANDOM_STATE)\n\ndef encode_data(X):\n    for feature in [\"Sex\", \"Embarked\", \"Name\", \"Ticket\"]:\n        if feature in X.columns:\n            X[feature] = X[feature].map({k: i for i, k in enumerate(X[feature].unique(), 0)})\n    return X\n    \ndef scale_data(X):\n    scaler = StandardScaler()\n    return scaler.fit_transform(X)\n\ndef train_model(X, y):\n    model = RandomForestClassifier(random_state = RANDOM_STATE)\n    model.fit(X, y)\n    return model\n\ndef capture_metrics(model, X_test, y_test):\n    prediction = model.predict(X_test)\n    proba_prediction = model.predict_proba(X_test)\n    \n    return [roc_auc_score(y_test, proba_prediction[:,1]),\n            accuracy_score(y_test, prediction),\n           precision_score(y_test, prediction, average = 'binary'),\n           recall_score(y_test, prediction, average = 'binary'),\n           f1_score(y_test, prediction, average = 'binary')]\n\ndef get_results(X, y, TECHNIQUE_NAME: str, TECHNIQUE_TYPE: str, scaled = False):\n    \n    start = time.time()\n\n    print(\"Encoding, Scaling and splitting...\")\n    X = encode_data(X)\n    if not scale_data:\n        X = scale_data(X)\n    X, X_test, y_train, y_test = split_data(X, y)\n    \n    print(\"Training...\")\n    model = train_model(X, y_train)\n    \n    print(\"Metrics...\")\n    metrics = capture_metrics(model, X_test, y_test)\n    \n    Technique.append(TECHNIQUE_NAME)\n    Type.append(TECHNIQUE_TYPE)\n    auc_score.append(metrics[0])\n    accuracy.append(metrics[1])\n    precision.append(metrics[2])\n    recall.append(metrics[3])\n    f1_Score.append(metrics[4])\n    \n    print(\"Complete!\")\n    \n    end = time.time()\n    time_taken = end - start\n    print(f\"Duration: {time_taken}\")\n    training_duration.append(time_taken)\n    \ndef differences(X, NAME: str):\n    temp = pd.DataFrame(columns = [f\"X{number}\" for number in range(len(data.drop(\"Survived\", axis = 1).columns))],\n            index = [\"Original\", f\"{NAME}\"])\n    \n    temp.loc[\"Original\"] = data.drop(\"Survived\", axis = 1).columns\n    temp.loc[f\"{NAME}\"] = [feature if feature in X.columns else \"DROPPED\" for feature in data.drop(\"Survived\", axis = 1).columns]\n    \n    temp[\"Total X\"] = [data.drop(\"Survived\", axis = 1).shape[1], X.shape[1]]\n    \n    n_dropped.append(temp[\"Total X\"].iloc[0] - temp[\"Total X\"].iloc[1])\n    features_kept.append(X.columns.to_list())\n    features_dropped.append([feature for feature in data.drop(\"Survived\", axis = 1).columns if feature not in X.columns])\n    \n    return temp.style.applymap(lambda value: 'background-color : red' if value == \"DROPPED\" else '')\n\ndef create_summary():\n    return pd.DataFrame({\"Technique\" : Technique, \"Type\": Type, \"Total dropped\": n_dropped, \"Features dropped\" : features_dropped,\n                         \"Features kept\":features_kept, \"AUC SCORE\" : auc_score, \"Accuracy\" : accuracy, \"Precision\" : precision,\n                         \"Recall\" : recall, \"F1 Score\" : f1_Score, \"Time to transform\": algorithm_duration,\n                         \"Training time\":training_duration})","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:35.792221Z","iopub.execute_input":"2023-08-20T18:54:35.792617Z","iopub.status.idle":"2023-08-20T18:54:36.258524Z","shell.execute_reply.started":"2023-08-20T18:54:35.792588Z","shell.execute_reply":"2023-08-20T18:54:36.257607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normal Data","metadata":{}},{"cell_type":"code","source":"differences(X, \"RAW Data\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:36.262302Z","iopub.execute_input":"2023-08-20T18:54:36.262632Z","iopub.status.idle":"2023-08-20T18:54:36.338159Z","shell.execute_reply.started":"2023-08-20T18:54:36.262597Z","shell.execute_reply":"2023-08-20T18:54:36.337008Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(X, y, TECHNIQUE_NAME = \"Original data\", TECHNIQUE_TYPE = \"Normal data\")\n\nalgorithm_duration.append(0.0)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:36.340359Z","iopub.execute_input":"2023-08-20T18:54:36.340782Z","iopub.status.idle":"2023-08-20T18:54:36.722402Z","shell.execute_reply.started":"2023-08-20T18:54:36.340746Z","shell.execute_reply":"2023-08-20T18:54:36.721217Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It's important to observe that despite being aware of the relatively minimal impact of the **\"PassengerId\"** and **\"Name\"** features on model performance, I intentionally retained these features within the dataset. The rationale behind this deliberate inclusion lies in the curiosity to witness how various feature selection techniques will recognize and subsequently eliminate these less influential attributes.","metadata":{}},{"cell_type":"markdown","source":"---\n# Filter Methods\n\nFilter methods are a category of feature selection techniques that involve evaluating the intrinsic characteristics of features with respect to the target variable. These methods do not involve training a machine learning model; instead, they focus on assessing statistical properties or relationships between features and the target variable. The goal of filter methods is to rank or select features based on their individual attributes, which can include measures of correlation, statistical significance, and information gain.\n\nFilter methods have some common characteristics:\n\n1. **Independence:** Filter methods assess the relevance of features independently of each other. They don't consider the interaction between features.\n\n2. **Speed:** These methods are generally computationally efficient, as they don't involve iterative model training.\n\n3. **Preprocessing:** Filter methods are typically applied as a preprocessing step before training a machine learning model.\n\n4. **Statistical Criteria:** They often use statistical metrics to quantify the relationship between each feature and the target variable.\n\n5. **Ranking:** Features are ranked based on their scores or metrics, and a predetermined number of top-ranking features are selected for further analysis.\n\nFilter methods can be useful for quickly identifying potentially relevant features before delving into more complex and computationally expensive feature selection techniques or model training. They can help in reducing the dimensionality of the dataset and improving model performance by focusing only on the most informative features.\n\nSince filter methods use data statistics to take decisions we might not see that many changes in the data. However, it's important to note that filter methods have limitations as well. They might miss out on interactions between features that collectively contribute to predictive power, and their rankings can be biased by the presence of irrelevant or noisy features. Therefore, filter methods are often used in combination with other feature selection techniques to achieve better results.","metadata":{}},{"cell_type":"markdown","source":"---\n## Filter: Constant\n\nThe \"constant\" filter method, also known as constant feature removal, is a simple and intuitive technique used in feature selection. In this method, features (columns) that have the same value for all instances in a dataset are identified and removed. These constant features, as the name suggests, do not provide any variability or useful information, making them irrelevant for predictive modeling tasks.\n\nConstant features can arise due to various reasons, such as data collection errors, missing values that were filled with the same value, or simply features that are inherently constant across the entire dataset.\n\nIt's important to note that while removing constant features is a straightforward approach, it should be used as a preliminary step in the feature selection process. It might not be sufficient for more complex datasets, and combining this method with other techniques can yield better results.","metadata":{"tags":[]}},{"cell_type":"code","source":"from feature_engine.selection import DropConstantFeatures\n\nalgorithm_start = time.time()\n\nselector =  DropConstantFeatures()\n\nSelected_X = selector.fit_transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:36.723773Z","iopub.execute_input":"2023-08-20T18:54:36.724225Z","iopub.status.idle":"2023-08-20T18:54:36.786788Z","shell.execute_reply.started":"2023-08-20T18:54:36.724187Z","shell.execute_reply":"2023-08-20T18:54:36.78579Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Constant\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:36.788274Z","iopub.execute_input":"2023-08-20T18:54:36.788941Z","iopub.status.idle":"2023-08-20T18:54:36.80993Z","shell.execute_reply.started":"2023-08-20T18:54:36.788901Z","shell.execute_reply":"2023-08-20T18:54:36.808727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Constant\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:36.811188Z","iopub.execute_input":"2023-08-20T18:54:36.811522Z","iopub.status.idle":"2023-08-20T18:54:37.186363Z","shell.execute_reply.started":"2023-08-20T18:54:36.811458Z","shell.execute_reply":"2023-08-20T18:54:37.185257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Filter: Quasi-Constant\n\nThe \"quasi-constant\" filter method, also known as quasi-constant feature removal, is an extension of the constant feature removal technique in feature selection. In this method, features that have a very low variance, indicating very little variability in their values across the dataset, are identified and considered for removal.\n\nWhile constant features have the same value for all instances in the dataset, quasi-constant features might have a few different values but are heavily skewed toward a single value. These features are also considered to be uninformative because they do not contribute much to the variability of the target variable.\n\nThe process of removing quasi-constant features is similar to that of constant features, but the criterion for identifying them involves variance or some other measure of dispersion. If a feature's variance falls below a certain threshold, it is considered quasi-constant and can be removed from the dataset.\n\nRemoving quasi-constant features offers similar benefits to removing constant features, including computational efficiency, model simplification, and the avoidance of overfitting. However, this method is more nuanced as it considers a range of values, allowing for features that are only slightly variable to also be addressed.\n\nAs with any feature selection technique, the decision to use the quasi-constant filter method should be made with careful consideration of the dataset and the specific problem at hand. Combining this method with other feature selection approaches can lead to a more comprehensive and effective feature selection strategy.","metadata":{}},{"cell_type":"code","source":"algorithm_start = time.time()\n\nQuasiConstantFeatures = []\n\nfor feature in X.columns:\n    \n    predominant = X[feature].value_counts(normalize = True).sort_values(ascending = False).values[0]\n    \n    if predominant > 0.998:\n        \n        QuasiConstantFeatures.append(feature)\n\nSelected_X = X.drop(labels = QuasiConstantFeatures, axis = 1)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:37.187741Z","iopub.execute_input":"2023-08-20T18:54:37.188073Z","iopub.status.idle":"2023-08-20T18:54:37.205529Z","shell.execute_reply.started":"2023-08-20T18:54:37.188044Z","shell.execute_reply":"2023-08-20T18:54:37.204625Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Quasi-Constant\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:37.206756Z","iopub.execute_input":"2023-08-20T18:54:37.207069Z","iopub.status.idle":"2023-08-20T18:54:37.225387Z","shell.execute_reply.started":"2023-08-20T18:54:37.207042Z","shell.execute_reply":"2023-08-20T18:54:37.224347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Quasi-Constant\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:37.226937Z","iopub.execute_input":"2023-08-20T18:54:37.227637Z","iopub.status.idle":"2023-08-20T18:54:37.601855Z","shell.execute_reply.started":"2023-08-20T18:54:37.227599Z","shell.execute_reply":"2023-08-20T18:54:37.600764Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Filter: Duplicated\n\nThe \"duplicated\" filter method, as the name suggests, is a feature selection technique that focuses on identifying and removing duplicated or highly correlated features from a dataset. Duplicated features are those that have the exact same values for all instances, while highly correlated features exhibit a strong linear relationship, potentially redundant in terms of the information they provide.\n\nThe process of using the duplicated filter method involves calculating a measure of similarity (such as Pearson's correlation coefficient) between pairs of features. If the similarity measure surpasses a certain threshold, the features are considered to be duplicated or highly correlated. At this point, one of the features is retained, while the others are removed.\n\nIt's important to exercise caution while applying the duplicated filter method, as not all correlated features need to be removed. Sometimes, correlated features might carry complementary information that can be beneficial for the model's performance. Careful consideration of the context and problem domain is crucial in deciding which features to retain and which to remove.\n\nIn practice, various programming libraries and tools offer functionalities to identify and address duplicated or highly correlated features, streamlining the process of preprocessing and feature selection.","metadata":{}},{"cell_type":"code","source":"algorithm_start = time.time()\n\nDuplicated_Features, Feature_holding = {}, []\n\nfor i in range(0, len(X.columns)):\n\n    FIRST_FEATURE = X.columns[i]\n    \n    if FIRST_FEATURE not in Feature_holding:\n    \n        Duplicated_Features[FIRST_FEATURE] = []\n\n        for SECOND_FEATURE in X.columns[i + 1:]:\n\n            if X[FIRST_FEATURE].equals(X[SECOND_FEATURE]):\n\n                Duplicated_Features[FIRST_FEATURE].append(SECOND_FEATURE)\n                Feature_holding.append(SECOND_FEATURE)\n \nSelected_X = X[Duplicated_Features.keys()]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:37.603273Z","iopub.execute_input":"2023-08-20T18:54:37.603628Z","iopub.status.idle":"2023-08-20T18:54:37.615191Z","shell.execute_reply.started":"2023-08-20T18:54:37.603598Z","shell.execute_reply":"2023-08-20T18:54:37.614059Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Duplicated\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:37.616684Z","iopub.execute_input":"2023-08-20T18:54:37.617075Z","iopub.status.idle":"2023-08-20T18:54:37.639715Z","shell.execute_reply.started":"2023-08-20T18:54:37.617047Z","shell.execute_reply":"2023-08-20T18:54:37.638699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Duplicated\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:37.64117Z","iopub.execute_input":"2023-08-20T18:54:37.642056Z","iopub.status.idle":"2023-08-20T18:54:38.011675Z","shell.execute_reply.started":"2023-08-20T18:54:37.642027Z","shell.execute_reply":"2023-08-20T18:54:38.010681Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Filter: Pearson\n\nThe \"Pearson correlation\" filter method is a feature selection technique that leverages the Pearson correlation coefficient to measure the linear relationship between pairs of continuous numerical features in a dataset. The Pearson correlation coefficient quantifies the strength and direction (positive or negative) of the linear relationship between two variables, ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no linear correlation.\n\nIn the context of feature selection, the Pearson correlation method is used to identify pairs of features that are highly correlated with each other. When two features are strongly correlated, they might be redundant, as they provide similar information to the model. In such cases, removing one of the correlated features can help reduce multicollinearity (correlation between predictor variables) and improve model interpretability and generalization.\n\nThe process of using the Pearson correlation filter method involves the following steps:\n\n1. **Calculate Correlations:** Compute the Pearson correlation coefficient for each pair of numerical features in the dataset.\n\n2. **Set a Threshold:** Choose a correlation threshold value (usually between -1 and 1) that determines the level of correlation deemed significant.\n\n3. **Identify Correlated Features:** Identify pairs of features that have correlation coefficients above the threshold. These pairs are considered candidates for removal.\n\n4. **Remove Features:** Depending on the context and the goal of feature selection, you can decide whether to remove one or both of the features in a correlated pair.\n\nIt's important to note that the Pearson correlation method is specifically suited for identifying linear relationships between continuous numerical variables. If your dataset contains categorical variables or non-linear relationships, the Pearson correlation might not capture the complete picture. In such cases, other correlation measures or techniques might be more appropriate.\n\nWhen using the Pearson correlation filter method, it's essential to consider the domain knowledge, the specific problem you're tackling, and the implications of removing correlated features. In some cases, removing correlated features can enhance model performance, while in other cases, it might inadvertently remove relevant information.","metadata":{}},{"cell_type":"code","source":"from feature_engine.selection import DropCorrelatedFeatures\n\nalgorithm_start = time.time()\n\nselector = DropCorrelatedFeatures(method = 'pearson')\n\nSelected_X = selector.fit_transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:38.013384Z","iopub.execute_input":"2023-08-20T18:54:38.014179Z","iopub.status.idle":"2023-08-20T18:54:38.026457Z","shell.execute_reply.started":"2023-08-20T18:54:38.014135Z","shell.execute_reply":"2023-08-20T18:54:38.025663Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Pearson\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:38.028564Z","iopub.execute_input":"2023-08-20T18:54:38.028905Z","iopub.status.idle":"2023-08-20T18:54:38.048866Z","shell.execute_reply.started":"2023-08-20T18:54:38.028866Z","shell.execute_reply":"2023-08-20T18:54:38.047688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Pearson\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:38.050459Z","iopub.execute_input":"2023-08-20T18:54:38.050981Z","iopub.status.idle":"2023-08-20T18:54:38.410342Z","shell.execute_reply.started":"2023-08-20T18:54:38.050952Z","shell.execute_reply":"2023-08-20T18:54:38.409226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Filter: Metric Based\n\nThe \"metric-based\" filter method, also known as metric-based feature selection, is a feature selection technique that involves evaluating features based on their individual relationships with a designated evaluation metric. This evaluation metric can be a scoring function that quantifies the performance of a machine learning model on a specific task, such as classification accuracy, F1-score, area under the ROC curve (AUC-ROC), or any other suitable measure.\n\nUnlike some other filter methods that use statistical properties or correlations, metric-based feature selection directly assesses the predictive power of each feature in relation to the chosen evaluation metric. The general process of metric-based feature selection involves the following steps:\n\n1. **Evaluation Metric Selection:** Choose an appropriate evaluation metric that aligns with the task and goal of your machine learning model (e.g., classification, regression).\n\n2. **Feature Scoring:** Calculate a score for each individual feature based on how well it contributes to improving the chosen evaluation metric. This scoring can involve training a machine learning model using each individual feature and measuring the resulting metric.\n\n3. **Feature Ranking or Selection:** Rank the features based on their scores, or select a predetermined number of top-scoring features for further analysis or modeling.\n\nMetric-based feature selection is particularly advantageous when the ultimate goal is model performance optimization on a specific task. By directly incorporating the evaluation metric in the feature selection process, this technique ensures that selected features align closely with the task's objectives. It can be especially useful when dealing with high-dimensional datasets, as it aids in pinpointing the most informative features while potentially reducing overfitting by ignoring less influential ones.\n\nHowever, metric-based feature selection might not be suitable for all situations. It heavily relies on the chosen evaluation metric, which means that features selected based on one metric might not perform optimally with a different metric. Additionally, it can be computationally expensive if a complex model needs to be trained for each feature.\n\nCareful consideration of the evaluation metric, dataset characteristics, and the potential trade-offs between model interpretability and performance is crucial when employing the metric-based filter method.","metadata":{}},{"cell_type":"code","source":"from feature_engine.selection import SelectBySingleFeaturePerformance\n\nalgorithm_start = time.time()\n\nselector = SelectBySingleFeaturePerformance(estimator = RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = -1))\n\nSelected_X = selector.fit_transform(X, y)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:38.411806Z","iopub.execute_input":"2023-08-20T18:54:38.412571Z","iopub.status.idle":"2023-08-20T18:54:45.503745Z","shell.execute_reply.started":"2023-08-20T18:54:38.41253Z","shell.execute_reply":"2023-08-20T18:54:45.502608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Metric Based\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:45.512424Z","iopub.execute_input":"2023-08-20T18:54:45.512814Z","iopub.status.idle":"2023-08-20T18:54:45.535327Z","shell.execute_reply.started":"2023-08-20T18:54:45.512782Z","shell.execute_reply":"2023-08-20T18:54:45.534551Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Metric Based\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:45.53642Z","iopub.execute_input":"2023-08-20T18:54:45.536979Z","iopub.status.idle":"2023-08-20T18:54:45.857194Z","shell.execute_reply.started":"2023-08-20T18:54:45.53695Z","shell.execute_reply":"2023-08-20T18:54:45.856043Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Filter: Mutual information\n\nThe \"Mutual Information\" (MI) filter method is a feature selection technique that quantifies the statistical dependence or information shared between two variables. In the context of feature selection, Mutual Information measures the amount of information one feature provides about another or, more specifically, about the target variable. It's often used to identify features that are informative for predicting the target variable in a classification or regression task.\n\nMutual Information is a measure derived from information theory, a field in mathematics and computer science that deals with quantifying the amount of information contained in data. It's particularly well-suited for identifying non-linear relationships and interactions between features and the target variable.\n\nThe general process of using Mutual Information for feature selection involves these steps:\n\n1. **Calculate Mutual Information:** Calculate the Mutual Information between each feature and the target variable. This involves assessing how much knowing the value of one feature reduces uncertainty about the target variable.\n\n2. **Rank or Select Features:** Rank the features based on their Mutual Information scores. Higher scores indicate stronger relationships with the target variable.\n\n3. **Set a Threshold:** Optionally, you can set a threshold to select a certain number of top-scoring features or choose features above a certain Mutual Information value.\n\nMutual Information is beneficial when dealing with datasets where linear correlation-based methods might not capture the full extent of feature relevance. It's particularly useful when dealing with categorical or discrete features. Additionally, Mutual Information can reveal non-linear relationships and interactions that other methods might overlook.\n\nWhile Mutual Information is a powerful technique, it's important to note that it has some limitations. For instance, it might not be suitable for high-dimensional datasets due to increased computational requirements. Additionally, the interpretability of Mutual Information scores might vary based on the nature of the data and the chosen problem.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import SelectKBest\n\nalgorithm_start = time.time()\n\nselector = SelectKBest(mutual_info_classif, k = int(X.shape[1] / 2)).fit(X, y)\n\nSelected_X = selector.transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:45.858686Z","iopub.execute_input":"2023-08-20T18:54:45.858998Z","iopub.status.idle":"2023-08-20T18:54:45.953508Z","shell.execute_reply.started":"2023-08-20T18:54:45.85897Z","shell.execute_reply":"2023-08-20T18:54:45.952403Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Mutual information\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:45.955025Z","iopub.execute_input":"2023-08-20T18:54:45.955359Z","iopub.status.idle":"2023-08-20T18:54:45.975693Z","shell.execute_reply.started":"2023-08-20T18:54:45.955329Z","shell.execute_reply":"2023-08-20T18:54:45.974545Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Mutual information\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:45.977157Z","iopub.execute_input":"2023-08-20T18:54:45.97775Z","iopub.status.idle":"2023-08-20T18:54:46.327831Z","shell.execute_reply.started":"2023-08-20T18:54:45.977719Z","shell.execute_reply":"2023-08-20T18:54:46.326517Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Filter: Chi-square\n\nThe \"Chi-Square\" filter method is a feature selection technique that is specifically designed for categorical target variables in classification tasks. It assesses the relationship between each categorical feature and the categorical target variable using the chi-square statistic, which measures the independence between two categorical variables.\n\nThe primary goal of the Chi-Square filter method is to identify features that are significantly associated with the target variable's different categories. It helps determine whether there is a statistically significant difference in the distribution of feature values across the categories of the target variable.\n\nThe general process of using the Chi-Square filter method involves the following steps:\n\n1. **Contingency Table:** Create a contingency table (also known as a cross-tabulation) that summarizes the frequency of each combination of feature values and target variable categories.\n\n2. **Expected Frequencies:** Calculate the expected frequencies for each cell in the contingency table under the assumption of independence between the feature and the target.\n\n3. **Chi-Square Statistic:** Calculate the chi-square statistic based on the observed and expected frequencies. This statistic quantifies the extent to which the observed frequencies deviate from the expected frequencies.\n\n4. **Degrees of Freedom:** Determine the degrees of freedom based on the dimensions of the contingency table.\n\n5. **Significance Test:** Perform a chi-square significance test to determine whether the chi-square statistic is statistically significant. This involves comparing the chi-square value to a critical value from the chi-square distribution.\n\n6. **Feature Selection:** Features with a significant chi-square statistic are considered relevant and are retained, while others may be discarded.\n\nChi-Square filter method is particularly suitable for datasets with categorical features and categorical target variables. It's commonly used in feature selection for text mining, sentiment analysis, and other classification tasks where both features and target variable are categorical.\n\nHowever, it's important to note that the Chi-Square filter method might not capture non-linear relationships or interactions between variables. Additionally, it assumes that the expected frequencies are not too small; otherwise, the chi-square test might be less reliable.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import chi2_contingency\n\nalgorithm_start = time.time()\n\np_values = []\n\nfor feature in X.columns:\n    c = pd.crosstab(y, X[feature])\n    p_value = chi2_contingency(c)[1]\n    p_values.append(p_value)\n    \nselected = pd.Series(p_values, index = X.columns).sort_values(ascending = True)[0:int(X.shape[1] / 2)].index\n\nSelected_X = X[selected]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:46.329057Z","iopub.execute_input":"2023-08-20T18:54:46.329359Z","iopub.status.idle":"2023-08-20T18:54:46.921895Z","shell.execute_reply.started":"2023-08-20T18:54:46.329333Z","shell.execute_reply":"2023-08-20T18:54:46.920586Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Chi-square\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:46.923451Z","iopub.execute_input":"2023-08-20T18:54:46.924286Z","iopub.status.idle":"2023-08-20T18:54:46.943989Z","shell.execute_reply.started":"2023-08-20T18:54:46.924254Z","shell.execute_reply":"2023-08-20T18:54:46.942537Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Chi-square\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:46.945374Z","iopub.execute_input":"2023-08-20T18:54:46.945821Z","iopub.status.idle":"2023-08-20T18:54:47.276376Z","shell.execute_reply.started":"2023-08-20T18:54:46.945791Z","shell.execute_reply":"2023-08-20T18:54:47.275217Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Filter: ANOVA\n\nThe \"ANOVA\" (Analysis of Variance) filter method is a feature selection technique that employs statistical hypothesis testing to assess the significance of the variation in a continuous numerical feature with respect to a categorical target variable. ANOVA is commonly used when dealing with regression or classification tasks where the target variable is categorical.\n\nThe primary objective of ANOVA in the context of feature selection is to determine whether there are statistically significant differences in the means of the continuous feature across different categories of the target variable. In other words, ANOVA helps identify features that exhibit variations in their values that are related to the categories of the target variable.\n\nThe general process of using ANOVA for feature selection involves the following steps:\n\n1. **Group Data:** Group the data based on the categories of the categorical target variable.\n\n2. **Calculate Variance:** Calculate the variance within each group, as well as the variance among the groups. This involves assessing how much the feature's values vary within each category group and how much they vary between different category groups.\n\n3. **Perform Hypothesis Test:** Use the F-test (a statistical test associated with ANOVA) to determine whether the variance among the groups is significantly larger than the variance within the groups. A significant F-statistic indicates that the feature's values are different across the categories of the target variable.\n\n4. **Rank or Select Features:** Rank the features based on their F-statistic values or select features with F-statistic values above a certain threshold.\n\nANOVA is effective when you suspect that there are significant variations in the feature values across different classes of the target variable. It can help identify features that contribute to the differences between categories, potentially improving the model's ability to discriminate between classes.\n\nHowever, ANOVA has some limitations. It assumes that the continuous feature follows a normal distribution, and it might not perform well if this assumption is violated. Additionally, ANOVA doesn't capture non-linear relationships between features and the target variable.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import f_classif, SelectKBest\n\nalgorithm_start = time.time()\n\nselector = SelectKBest(f_classif, k = int(X.shape[1] / 2)).fit(X, y)\n\nSelected_X = selector.transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:47.27817Z","iopub.execute_input":"2023-08-20T18:54:47.278601Z","iopub.status.idle":"2023-08-20T18:54:47.292103Z","shell.execute_reply.started":"2023-08-20T18:54:47.278563Z","shell.execute_reply":"2023-08-20T18:54:47.29078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"ANOVA\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:47.293606Z","iopub.execute_input":"2023-08-20T18:54:47.293983Z","iopub.status.idle":"2023-08-20T18:54:47.314067Z","shell.execute_reply.started":"2023-08-20T18:54:47.293954Z","shell.execute_reply":"2023-08-20T18:54:47.312813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"ANOVA\", TECHNIQUE_TYPE = \"Filter\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T18:54:47.315146Z","iopub.execute_input":"2023-08-20T18:54:47.315522Z","iopub.status.idle":"2023-08-20T18:54:47.631299Z","shell.execute_reply.started":"2023-08-20T18:54:47.315457Z","shell.execute_reply":"2023-08-20T18:54:47.630254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# Wrapper Methods\n\nWrapper methods are a class of feature selection techniques that involve using a machine learning model's performance as a criterion to evaluate and select features. Unlike filter methods, which assess features independently of the model, wrapper methods involve training and evaluating the model multiple times with different subsets of features. The goal is to identify the optimal subset of features that yields the best model performance according to a specified evaluation metric.\n\nWrapper methods typically have the following characteristics:\n\n1. **Model-Dependent:** Wrapper methods rely on a specific machine learning algorithm as part of their evaluation process. The model is trained and evaluated iteratively with different subsets of features.\n\n2. **Feature Subsets:** These methods explore various combinations of features and evaluate their impact on model performance.\n\n3. **Computationally Expensive:** Because wrapper methods require training the model multiple times, they can be computationally intensive, especially with large datasets or complex models.\n\n4. **Higher Performance:** Wrapper methods aim to find feature subsets that directly improve the performance of the chosen machine learning model.\n\n5. **Model Overfitting:** If not used carefully, wrapper methods can lead to overfitting, as they optimize feature subsets based on the specific model's behavior on the training data.\n\nWrapper methods can provide a more accurate assessment of the feature's relevance to the specific model's performance. However, they come with a higher computational cost due to the repeated model training. These methods can be beneficial when filter methods don't fully capture the complexity of the relationship between features and the target variable.\n\nWhen using wrapper methods, it's important to select an appropriate evaluation metric, be cautious about potential overfitting, and be prepared for longer computation times compared to filter methods. Additionally, as model-dependent techniques, wrapper methods might not generalize well to different models or tasks.","metadata":{}},{"cell_type":"markdown","source":"---\n## Wrapper: Step Forward Selection\n\nThe \"step forward selection\" wrapper method is a feature selection technique that involves iteratively building a model by progressively adding one feature at a time to the feature subset. This method aims to identify the optimal subset of features that maximizes the performance of a chosen machine learning model.\n\nThe general process of step forward selection involves the following steps:\n\n1. **Initialization:** Start with an empty set of selected features.\n\n2. **Feature Evaluation:** For each feature not yet selected, train the machine learning model using the currently selected features along with the feature under consideration. Evaluate the model's performance using a predefined evaluation metric.\n\n3. **Feature Selection:** Select the feature that, when added to the currently selected features, results in the highest improvement in model performance according to the evaluation metric.\n\n4. **Iteration:** Repeat steps 2 and 3 until a stopping criterion is met. This criterion could be reaching a desired number of features or observing a decline in performance improvement.\n\n5. **Final Model:** Train the machine learning model using the selected features and evaluate its performance on a separate validation or test dataset.\n\nStep forward selection offers several advantages:\n\n- **Customization:** This method allows for fine-grained control over the selection process, as each iteration involves evaluating the performance gain from adding a specific feature.\n\n- **Model Performance:** By iteratively adding features that contribute the most to model performance, step forward selection often leads to improved model accuracy.\n\nHowever, there are also some considerations:\n\n- **Computational Intensity:** Step forward selection can be computationally expensive, as it involves training the model multiple times for each feature addition.\n\n- **Overfitting:** There's a risk of overfitting if the stopping criterion is not carefully chosen, as the method might select features that perform well on the training data but not on unseen data.\n\n- **Limited Exploration:** Step forward selection doesn't backtrack or revisit features, potentially missing out on globally optimal feature subsets.\n\nWhen using step forward selection, it's important to set the evaluation metric appropriately, manage computational resources, and be mindful of the potential for overfitting.","metadata":{}},{"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector\n\nalgorithm_start = time.time()\n\nselector = SequentialFeatureSelector(RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = -1), \n           k_features = int(X.shape[1] / 2), \n           forward = True, \n           floating = False, \n           verbose = 0,\n           scoring = 'accuracy',\n           cv = 3)\n\nselector = selector.fit(X, y)\n\nSelected_X = X[X.columns[list(selector.k_feature_idx_)]]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:54:47.632845Z","iopub.execute_input":"2023-08-20T18:54:47.633187Z","iopub.status.idle":"2023-08-20T18:55:10.559533Z","shell.execute_reply.started":"2023-08-20T18:54:47.633158Z","shell.execute_reply":"2023-08-20T18:55:10.558393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Step Forward Selection\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:55:10.560846Z","iopub.execute_input":"2023-08-20T18:55:10.561634Z","iopub.status.idle":"2023-08-20T18:55:10.58004Z","shell.execute_reply.started":"2023-08-20T18:55:10.561603Z","shell.execute_reply":"2023-08-20T18:55:10.579066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Step Forward Selection\", TECHNIQUE_TYPE = \"Wrapper\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:55:10.581578Z","iopub.execute_input":"2023-08-20T18:55:10.581889Z","iopub.status.idle":"2023-08-20T18:55:10.90159Z","shell.execute_reply.started":"2023-08-20T18:55:10.581851Z","shell.execute_reply":"2023-08-20T18:55:10.900495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Wrapper: Step Backward Selection\n\nThe \"step backward selection\" wrapper method is a feature selection technique that is the inverse of the step forward selection method. Instead of starting with an empty set of features and adding one feature at a time, step backward selection begins with all features included and iteratively removes one feature at a time to find the optimal subset of features that maximizes model performance.\n\nThe general process of step backward selection involves the following steps:\n\n1. **Initialization:** Start with all available features included in the feature subset.\n\n2. **Feature Evaluation:** Train the machine learning model using the current subset of features and evaluate its performance using a specified evaluation metric.\n\n3. **Feature Removal:** Remove the feature that, when excluded from the subset, leads to the highest improvement in model performance according to the evaluation metric.\n\n4. **Iteration:** Repeat steps 2 and 3 until a stopping criterion is met. This criterion could be reaching a desired number of features or observing a decline in performance improvement.\n\n5. **Final Model:** Train the machine learning model using the selected features and evaluate its performance on a separate validation or test dataset.\n\nStep backward selection shares similarities with step forward selection but takes the approach of starting with all features and iteratively removing them based on their impact on model performance.\n\nStep backward selection offers similar advantages and considerations as step forward selection:\n\n- **Customization:** Step backward selection allows for fine-tuning the selection process by evaluating the performance impact of removing each feature.\n\n- **Model Performance:** By iteratively removing features that contribute the least to model performance, step backward selection often leads to improved model accuracy.\n\n- **Computational Intensity:** Like step forward selection, step backward selection can be computationally demanding due to the repeated model training.\n\n- **Overfitting:** There's a risk of overfitting if the stopping criterion is not well-defined, as the method might end up removing features that could generalize well to unseen data.\n\n- **Limited Exploration:** Step backward selection doesn't backtrack or reconsider features, potentially missing out on globally optimal feature subsets.\n\nWhen using step backward selection, similar precautions should be taken with regard to setting the evaluation metric, managing computational resources, and addressing the possibility of overfitting.","metadata":{}},{"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector\n\nalgorithm_start = time.time()\n\nselector = SequentialFeatureSelector(RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = -1), \n           k_features = int(X.shape[1] / 2), forward = False, floating = False, verbose = 0, \n           scoring = 'roc_auc', cv = 3)\n\nselector = selector.fit(X, y)\n\nSelected_X = X[X.columns[list(selector.k_feature_idx_)]]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:55:10.902977Z","iopub.execute_input":"2023-08-20T18:55:10.903302Z","iopub.status.idle":"2023-08-20T18:55:35.17257Z","shell.execute_reply.started":"2023-08-20T18:55:10.903274Z","shell.execute_reply":"2023-08-20T18:55:35.170859Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Step Backward Selection\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:55:35.174013Z","iopub.execute_input":"2023-08-20T18:55:35.174424Z","iopub.status.idle":"2023-08-20T18:55:35.19987Z","shell.execute_reply.started":"2023-08-20T18:55:35.174387Z","shell.execute_reply":"2023-08-20T18:55:35.198568Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Step Backward Selection\", TECHNIQUE_TYPE = \"Wrapper\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:55:35.201247Z","iopub.execute_input":"2023-08-20T18:55:35.201572Z","iopub.status.idle":"2023-08-20T18:55:35.569472Z","shell.execute_reply.started":"2023-08-20T18:55:35.201544Z","shell.execute_reply":"2023-08-20T18:55:35.568339Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Wrapper: Exhaustive Selection\n\nExhaustive selection, also known as \"exhaustive search,\" is a comprehensive wrapper method for feature selection in machine learning. This technique involves evaluating all possible subsets of features from the dataset and selecting the subset that results in the best model performance according to a specified evaluation metric. While exhaustive selection is very thorough, it can become computationally expensive and time-consuming, especially for datasets with a large number of features.\n\nThe general process of exhaustive selection involves the following steps:\n\n1. **Subset Generation:** Generate all possible combinations of features from the dataset, including subsets of different sizes.\n\n2. **Model Training and Evaluation:** For each generated subset, train a machine learning model using the selected features and evaluate its performance using a predefined evaluation metric.\n\n3. **Best Subset Identification:** Identify the subset of features that yields the highest model performance according to the evaluation metric.\n\n4. **Final Model:** Train the machine learning model using the selected best subset of features and evaluate its performance on a separate validation or test dataset.\n\nExhaustive selection is appealing because it guarantees finding the optimal subset of features that maximizes the chosen model's performance. However, it comes with several significant considerations:\n\n- **Computational Intensity:** The number of possible subsets grows exponentially with the number of features. As a result, exhaustive selection can be extremely computationally expensive, especially for datasets with a large number of features.\n\n- **Resource Demands:** This method requires a substantial amount of computational resources and time, making it less practical for real-world applications with tight deadlines.\n\n- **Curse of Dimensionality:** As the number of features increases, the time and resources required for exhaustive selection grow exponentially, which can make it infeasible for high-dimensional datasets.\n\n- **Overfitting:** Similar to other wrapper methods, there's a potential for overfitting if the stopping criterion is not well-defined, as the method might optimize the model specifically for the training data.\n\nExhaustive selection is most suitable for scenarios where computational resources are not a constraint, and achieving the absolute best model performance is the primary goal. In practice, due to its high computational demands, it might not be feasible for large datasets with many features. As an alternative, it can be beneficial to explore more efficient wrapper methods or a combination of both filter and wrapper methods to strike a balance between accuracy and computational cost.","metadata":{"tags":[]}},{"cell_type":"code","source":"from mlxtend.feature_selection import ExhaustiveFeatureSelector\n\nalgorithm_start = time.time()\n\nselector = ExhaustiveFeatureSelector(RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = -1),\n          scoring = 'roc_auc', cv = 3, min_features = 3, max_features = 7)\n\nselector = selector.fit(X, y)\n\nSelected_X = X[X.columns[list(selector.best_idx_)]]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T18:55:35.571004Z","iopub.execute_input":"2023-08-20T18:55:35.571426Z","iopub.status.idle":"2023-08-20T19:04:42.889879Z","shell.execute_reply.started":"2023-08-20T18:55:35.571388Z","shell.execute_reply":"2023-08-20T19:04:42.888669Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Exhaustive Selection\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:42.891528Z","iopub.execute_input":"2023-08-20T19:04:42.891879Z","iopub.status.idle":"2023-08-20T19:04:42.91239Z","shell.execute_reply.started":"2023-08-20T19:04:42.891849Z","shell.execute_reply":"2023-08-20T19:04:42.911229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Exhaustive Selection\", TECHNIQUE_TYPE = \"Wrapper\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:42.914035Z","iopub.execute_input":"2023-08-20T19:04:42.91441Z","iopub.status.idle":"2023-08-20T19:04:43.278548Z","shell.execute_reply.started":"2023-08-20T19:04:42.914367Z","shell.execute_reply":"2023-08-20T19:04:43.277336Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Wrapper: Logistic\n\nThe \"logistic regression\" wrapper method is a feature selection technique that involves using a logistic regression model as the evaluation criterion to select the most relevant features for a classification task. In this approach, a logistic regression model is trained iteratively with different subsets of features, and the subset that results in the best model performance is selected.\n\nThe general process of using logistic regression as a wrapper method for feature selection involves the following steps:\n\n1. **Initialization:** Start with an empty set of selected features.\n\n2. **Feature Evaluation:** For each feature not yet selected, train a logistic regression model using the currently selected features along with the feature under consideration. Evaluate the model's performance using a predefined evaluation metric, such as accuracy or F1-score.\n\n3. **Feature Selection:** Select the feature that, when added to the currently selected features, leads to the highest improvement in model performance according to the evaluation metric.\n\n4. **Iteration:** Repeat steps 2 and 3 until a stopping criterion is met. This criterion could be reaching a desired number of features or observing a decline in performance improvement.\n\n5. **Final Model:** Train the logistic regression model using the selected features and evaluate its performance on a separate validation or test dataset.\n\nUsing logistic regression as a wrapper method for feature selection has several benefits:\n\n- **Model Compatibility:** Logistic regression is a natural choice for classification tasks, making it a suitable model for evaluating feature relevance in a classification context.\n\n- **Interpretability:** Logistic regression provides interpretable coefficients for each selected feature, aiding in understanding their impact on the model's predictions.\n\n- **Regularization:** Depending on the implementation, logistic regression can automatically perform feature selection by applying regularization techniques like L1 regularization (Lasso), which encourages some feature coefficients to be exactly zero.\n\nHowever, it's important to consider the following:\n\n- **Computational Cost:** Repeatedly training logistic regression models can be computationally demanding, especially with large datasets.\n\n- **Feature Dependencies:** Logistic regression assumes linear relationships between features and the target variable. Non-linear relationships might not be well-captured by this method.\n\n- **Overfitting:** Care should be taken to avoid overfitting by using cross-validation, proper stopping criteria, and regularization if applicable.\n\nLogistic regression-based wrapper methods are most beneficial when logistic regression is a suitable model for the task and when feature interpretability and model performance are key priorities. In practice, combining wrapper methods with other techniques can help strike a balance between accuracy, computational cost, and model complexity.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nalgorithm_start = time.time()\n\n# NOTE: Features need to be scaled\nScaled_X = scale_data(X)\n\nselector = SelectFromModel(LogisticRegression(random_state = RANDOM_STATE))\n\nselector.fit(Scaled_X, y)\n\nSelected_X = X[X.columns[(selector.get_support())]]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:43.279885Z","iopub.execute_input":"2023-08-20T19:04:43.280215Z","iopub.status.idle":"2023-08-20T19:04:43.308707Z","shell.execute_reply.started":"2023-08-20T19:04:43.280188Z","shell.execute_reply":"2023-08-20T19:04:43.307516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Logistic\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:43.310081Z","iopub.execute_input":"2023-08-20T19:04:43.310383Z","iopub.status.idle":"2023-08-20T19:04:43.330889Z","shell.execute_reply.started":"2023-08-20T19:04:43.310357Z","shell.execute_reply":"2023-08-20T19:04:43.329676Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Logistic\", TECHNIQUE_TYPE = \"Wrapper\", scaled = True)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:43.332529Z","iopub.execute_input":"2023-08-20T19:04:43.332867Z","iopub.status.idle":"2023-08-20T19:04:43.64806Z","shell.execute_reply.started":"2023-08-20T19:04:43.332839Z","shell.execute_reply":"2023-08-20T19:04:43.647018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Wrapper: Lasso regularization\n\nThe \"Lasso\" (Least Absolute Shrinkage and Selection Operator) regularization, often referred to as L1 regularization, is a feature selection technique that is applied as part of a machine learning model's training process. Lasso adds a penalty term to the linear regression or logistic regression loss function to encourage some feature coefficients to become exactly zero. As a result, Lasso not only performs regression or classification but also inherently performs feature selection by reducing the impact of less important features.\n\nThe general concept of Lasso regularization involves these key aspects:\n\n1. **Regularization Term:** Lasso adds a penalty term to the loss function during model training. The penalty is proportional to the absolute value of the coefficients of the features.\n\n2. **Shrinking Coefficients:** The penalty term encourages the optimization algorithm to minimize the sum of the absolute values of the coefficients. This shrinking effect results in some coefficients becoming exactly zero.\n\n3. **Sparse Model:** The zero coefficients indicate that the corresponding features have been effectively excluded from the model, effectively performing feature selection.\n\nLasso regularization is particularly suitable when there are many features, some of which might be irrelevant or redundant. It automatically identifies and selects the most relevant features for the task, resulting in a more interpretable and potentially better-performing model.\n\nLasso regularization is advantageous for the following reasons:\n\n- **Feature Selection:** Lasso naturally performs feature selection by driving some coefficients to zero, effectively excluding irrelevant or less important features.\n\n- **Interpretability:** The model's sparsity due to zero coefficients leads to a more interpretable model.\n\n- **Dimensionality Reduction:** Lasso helps reduce the dimensionality of the problem by excluding irrelevant features.\n\nHowever, there are also considerations:\n\n- **Feature Dependencies:** Lasso might arbitrarily select one feature from a group of highly correlated features while setting the others to zero.\n\n- **Regularization Strength:** The regularization strength parameter should be appropriately chosen to control the balance between fitting the data and feature selection.\n\n- **Model Sensitivity:** Lasso's effectiveness depends on the data and problem at hand. The choice of regularization strength and the data's characteristics can impact its performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\nalgorithm_start = time.time()\n\n# NOTE: Features need to be scaled\nScaled_X = scale_data(X)\n\nselector = SelectFromModel(LogisticRegression(C = 0.5, penalty = 'l1', solver = 'liblinear', random_state = RANDOM_STATE))\n\nselector.fit(Scaled_X, y)\n\nSelected_X = selector.transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:43.649723Z","iopub.execute_input":"2023-08-20T19:04:43.650048Z","iopub.status.idle":"2023-08-20T19:04:43.66867Z","shell.execute_reply.started":"2023-08-20T19:04:43.650021Z","shell.execute_reply":"2023-08-20T19:04:43.667879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Lasso Regularization\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:43.669694Z","iopub.execute_input":"2023-08-20T19:04:43.670613Z","iopub.status.idle":"2023-08-20T19:04:43.689236Z","shell.execute_reply.started":"2023-08-20T19:04:43.670583Z","shell.execute_reply":"2023-08-20T19:04:43.688049Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Lasso Regularization\", TECHNIQUE_TYPE = \"Wrapper\", scaled = True)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:43.690586Z","iopub.execute_input":"2023-08-20T19:04:43.690872Z","iopub.status.idle":"2023-08-20T19:04:44.05149Z","shell.execute_reply.started":"2023-08-20T19:04:43.690847Z","shell.execute_reply":"2023-08-20T19:04:44.050481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Wrapper: Tree Importance \n\nThe \"Tree Importance\" wrapper method, often referred to as feature importance derived from decision trees or ensemble methods like Random Forests, is a feature selection technique that evaluates the significance of features based on their contribution to the predictive accuracy of a machine learning model.\n\nTree Importance is typically used with decision tree-based models or ensemble techniques like Random Forests or Gradient Boosting. These models inherently assess the importance of features during the training process and provide a quantitative measure of how much each feature contributes to improving the model's performance.\n\nThe general process of using Tree Importance for feature selection involves these steps:\n\n1. **Model Training:** Train a decision tree-based model or an ensemble model (e.g., Random Forest, Gradient Boosting) using the entire set of features and the corresponding target variable.\n\n2. **Feature Importance Calculation:** Extract the feature importance scores from the trained model. These scores indicate how much each feature contributes to the model's accuracy.\n\n3. **Rank or Select Features:** Rank the features based on their importance scores or select a predetermined number of top-scoring features.\n\n4. **Final Model:** Train the machine learning model using the selected features and evaluate its performance on a separate validation or test dataset.\n\nTree Importance is advantageous for several reasons:\n\n- **Inherent Feature Evaluation:** Decision tree-based models naturally assess feature importance during their training process. This eliminates the need for separate feature selection steps.\n\n- **Non-linearity and Interactions:** Tree-based models can capture non-linear relationships and interactions between features, making them suitable for tasks where such patterns are present.\n\n- **Model Performance:** Features contributing most to the model's predictive accuracy are retained, potentially leading to improved model performance.\n\nHowever, it's essential to consider:\n\n- **Bias Towards Strong Predictors:** Tree Importance might emphasize strong predictors, potentially overlooking relevant but weaker predictors.\n\n- **Data Sensitivity:** The importance of features can vary based on the dataset, problem, and model chosen.\n\n- **Model Choice:** Tree Importance is most applicable to decision tree-based models and ensembles. For linear models, other feature selection methods might be more suitable.","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nalgorithm_start = time.time()\n\nselector = SelectFromModel(RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = -1))\n\nselector.fit(X, y)\n\nSelected_X = X[X.columns[(selector.get_support())]]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:44.052779Z","iopub.execute_input":"2023-08-20T19:04:44.053092Z","iopub.status.idle":"2023-08-20T19:04:44.428932Z","shell.execute_reply.started":"2023-08-20T19:04:44.053065Z","shell.execute_reply":"2023-08-20T19:04:44.42803Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Tree Importance\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:44.43018Z","iopub.execute_input":"2023-08-20T19:04:44.430728Z","iopub.status.idle":"2023-08-20T19:04:44.449819Z","shell.execute_reply.started":"2023-08-20T19:04:44.430698Z","shell.execute_reply":"2023-08-20T19:04:44.44866Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Tree Importance\", TECHNIQUE_TYPE = \"Wrapper\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:44.450966Z","iopub.execute_input":"2023-08-20T19:04:44.451288Z","iopub.status.idle":"2023-08-20T19:04:44.833055Z","shell.execute_reply.started":"2023-08-20T19:04:44.45126Z","shell.execute_reply":"2023-08-20T19:04:44.831981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Wrapper: Recursive Selection\n\nThe \"Recursive Selection\" wrapper method is a feature selection technique that involves recursively training a machine learning model on different subsets of features and evaluating the model's performance to determine the optimal subset. It's a sequential process that iteratively adds or removes features based on their impact on model performance.\n\nThe Recursive Selection method can be further categorized into two main subtypes: Recursive Feature Elimination (RFE) and Recursive Feature Addition (RFA).\n\n1. **Recursive Feature Elimination (RFE):** In RFE, the process starts with all features included and iteratively removes the least important feature(s) based on a certain criterion. The model is retrained each time, and the least important features are pruned.\n\n2. **Recursive Feature Addition (RFA):** In RFA, the process begins with no features included and iteratively adds the most important feature(s) based on a certain criterion. The model is retrained with the added features at each step.\n\nThe general process of Recursive Selection involves these steps:\n\n1. **Initialization:** Start with either all features included (for RFE) or no features included (for RFA).\n\n2. **Model Training and Evaluation:** Train a machine learning model using the selected features and evaluate its performance using a predefined evaluation metric.\n\n3. **Feature Adjustment:** Depending on the subtype (RFE or RFA), either remove the least important feature or add the most important feature.\n\n4. **Iteration:** Repeat steps 2 and 3 for a predetermined number of iterations or until a stopping criterion is met.\n\n5. **Final Model:** Train the machine learning model using the selected features and evaluate its performance on a separate validation or test dataset.\n\nRecursive Selection offers several benefits:\n\n- **Thorough Exploration:** Recursive Selection method explores different subsets of features, systematically assessing their impact on model performance.\n\n- **Customization:** The process can be customized by selecting the number of features to include or exclude in each iteration.\n\n- **Feature Ranking:** The selection process inherently ranks features based on their importance, aiding in understanding feature relevance.\n\nHowever, there are also considerations:\n\n- **Computational Demand:** Repeatedly training and evaluating models can be computationally intensive.\n\n- **Overfitting:** Care should be taken to avoid overfitting, particularly if the stopping criterion is not well-defined.\n\n- **Model Choice:** Recursive Selection is applicable to various machine learning models, but the choice of the base model affects its performance.\n\nRecursive Selection is most suitable when a thorough exploration of feature subsets is desired, and when the computational resources are sufficient to handle the repeated model training. It can lead to improved model performance and interpretability by identifying the most relevant subset of features.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\n\nalgorithm_start = time.time()\n\nselector = RFE(RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = -1), n_features_to_select = int(X.shape[1] / 2))\n\nselector.fit(X, y)\n\nSelected_X = X[X.columns[(selector.get_support())]]\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:44.834431Z","iopub.execute_input":"2023-08-20T19:04:44.834978Z","iopub.status.idle":"2023-08-20T19:04:47.030218Z","shell.execute_reply.started":"2023-08-20T19:04:44.834943Z","shell.execute_reply":"2023-08-20T19:04:47.029312Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Recursive Selection\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:47.031402Z","iopub.execute_input":"2023-08-20T19:04:47.031849Z","iopub.status.idle":"2023-08-20T19:04:47.052126Z","shell.execute_reply.started":"2023-08-20T19:04:47.0318Z","shell.execute_reply":"2023-08-20T19:04:47.05092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Recursive Selection\", TECHNIQUE_TYPE = \"Wrapper\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:47.05394Z","iopub.execute_input":"2023-08-20T19:04:47.054409Z","iopub.status.idle":"2023-08-20T19:04:47.423436Z","shell.execute_reply.started":"2023-08-20T19:04:47.054369Z","shell.execute_reply":"2023-08-20T19:04:47.422492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n# Hybrid Methods\n\nHybrid methods in feature selection are approaches that combine elements from different types of feature selection techniques, such as filter and wrapper methods, to take advantage of their strengths while mitigating their weaknesses. Hybrid methods aim to provide a more comprehensive and balanced approach to selecting relevant features for a machine learning model. They typically involve integrating different steps or components from multiple techniques to achieve better feature selection results.","metadata":{}},{"cell_type":"markdown","source":"---\n## Hybrid: Feature Shuffling\n\nFeature shuffling is not commonly referred to as a \"hybrid method\" in the traditional sense of combining different types of feature selection techniques. Instead, it's a technique used for assessing the importance of individual features in a machine learning model, particularly in the context of evaluating feature relevance and understanding their impact on model predictions.\n\nFeature shuffling involves a simple yet effective process:\n\n1. **Initial Model Training:** Train a machine learning model using the original dataset, including all features.\n\n2. **Feature Importance Calculation:** Calculate the baseline feature importance scores using the trained model. These scores indicate the contribution of each feature to the model's performance.\n\n3. **Shuffling:** Randomly shuffle the values of a specific feature while keeping the target variable values intact. This destroys any genuine relationship between the feature and the target.\n\n4. **Model Evaluation:** Reevaluate the model's performance using the shuffled feature. The drop in performance indicates the significance of the shuffled feature.\n\n5. **Repeat:** Repeat steps 3 and 4 for each feature to assess the impact of shuffling on the model's performance.\n\nBy comparing the drop in model performance when a feature is shuffled with its baseline importance score, one can infer the relative importance of that feature. Features that, when shuffled, lead to a significant drop in performance are likely to be important for the model's predictions.\n\nWhile feature shuffling is a valuable technique for assessing feature importance and understanding their contributions, it's not a hybrid method in the traditional sense, as it doesn't combine elements from different feature selection techniques. It's a technique on its own that provides insights into feature relevance and can be a part of a larger toolkit for feature selection and model interpretation.","metadata":{}},{"cell_type":"code","source":"from feature_engine.selection import SelectByShuffling\n\nalgorithm_start = time.time()\n\nrandom_forest = RandomForestClassifier(random_state = RANDOM_STATE, n_jobs = -1)\n\nselector = SelectByShuffling(estimator = random_forest, random_state = RANDOM_STATE, threshold = 0)\n\nselector.fit(X, y)\n\nSelected_X = selector.transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:47.424593Z","iopub.execute_input":"2023-08-20T19:04:47.424887Z","iopub.status.idle":"2023-08-20T19:04:49.083517Z","shell.execute_reply.started":"2023-08-20T19:04:47.424861Z","shell.execute_reply":"2023-08-20T19:04:49.082372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Feature Shuffling\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:49.085001Z","iopub.execute_input":"2023-08-20T19:04:49.085806Z","iopub.status.idle":"2023-08-20T19:04:49.105924Z","shell.execute_reply.started":"2023-08-20T19:04:49.085766Z","shell.execute_reply":"2023-08-20T19:04:49.104838Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Feature Shuffling\", TECHNIQUE_TYPE = \"Hybrid\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:49.107407Z","iopub.execute_input":"2023-08-20T19:04:49.108318Z","iopub.status.idle":"2023-08-20T19:04:49.389612Z","shell.execute_reply.started":"2023-08-20T19:04:49.108284Z","shell.execute_reply":"2023-08-20T19:04:49.388492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Hybrid: Recursive Feature Elimination\n\nRecursive Feature Elimination (RFE) is a specific feature selection technique that often falls into the category of wrapper methods, as it involves using a machine learning model's performance as a criterion to select features. However, RFE can also be considered a hybrid method due to its iterative nature and its incorporation of filter-like characteristics.\n\nThe general process of Recursive Feature Elimination involves the following steps:\n\n1. **Model Training and Evaluation:** Train a machine learning model using all available features and evaluate its performance using a predefined evaluation metric.\n\n2. **Feature Ranking:** Rank the features based on their importance scores obtained from the trained model.\n\n3. **Feature Elimination:** Remove the feature with the lowest importance score from the dataset.\n\n4. **Model Retraining:** Train the machine learning model again, this time using the remaining features.\n\n5. **Iteration:** Repeat steps 2-4 until a stopping criterion is met, such as reaching a desired number of features or observing a decline in model performance.\n\n6. **Final Model:** Train the machine learning model using the selected subset of features and evaluate its performance on a separate validation or test dataset.\n\nRFE combines elements of both filter and wrapper methods:\n\n- **Filter-Like Aspect:** In each iteration, RFE evaluates the importance of features and eliminates the least important one, which is reminiscent of filter methods' ranking or scoring process.\n\n- **Wrapper-Like Aspect:** RFE iteratively trains and evaluates a machine learning model using different subsets of features, similar to the wrapper methods' approach.\n\nRFE is considered a hybrid method because it takes advantage of a machine learning model's predictive ability to assess feature importance, while also implementing an iterative process to refine the feature subset. It provides a balance between efficiency and effectiveness by iteratively eliminating less important features based on their contribution to the model's performance.\n\nWhile RFE is effective, it's important to consider the computational demands, the choice of the base model, and the potential for overfitting during the selection process. It's particularly beneficial when interpretability and model performance are both important considerations.","metadata":{}},{"cell_type":"code","source":"from feature_engine.selection import RecursiveFeatureElimination\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nalgorithm_start = time.time()\n\nGradient_Boosting = GradientBoostingClassifier(random_state = RANDOM_STATE)\n\nselector = RecursiveFeatureElimination(estimator = Gradient_Boosting, threshold = 0.0001)\n\nselector.fit(X, y)\n\nSelected_X = selector.transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:49.391141Z","iopub.execute_input":"2023-08-20T19:04:49.391985Z","iopub.status.idle":"2023-08-20T19:04:56.806771Z","shell.execute_reply.started":"2023-08-20T19:04:49.391947Z","shell.execute_reply":"2023-08-20T19:04:56.805767Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Recursive Feature Elimination\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:56.8081Z","iopub.execute_input":"2023-08-20T19:04:56.808506Z","iopub.status.idle":"2023-08-20T19:04:56.8301Z","shell.execute_reply.started":"2023-08-20T19:04:56.808452Z","shell.execute_reply":"2023-08-20T19:04:56.829062Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Recursive Feature Elimination\", TECHNIQUE_TYPE = \"Hybrid\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:56.831559Z","iopub.execute_input":"2023-08-20T19:04:56.832284Z","iopub.status.idle":"2023-08-20T19:04:57.175944Z","shell.execute_reply.started":"2023-08-20T19:04:56.832253Z","shell.execute_reply":"2023-08-20T19:04:57.174715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Hybrid: Recursive Feature Addition\n\nRecursive Feature Addition (RFA) is a feature selection technique that also falls under the wrapper methods category while having characteristics of hybrid methods due to its iterative approach. Similar to Recursive Feature Elimination (RFE), RFA involves training and evaluating a machine learning model on different subsets of features. However, RFA follows a different strategy by iteratively adding features based on their importance to the model's performance.\n\nThe general process of Recursive Feature Addition involves the following steps:\n\n1. **Model Training and Evaluation:** Train a machine learning model using no features and evaluate its performance using a predefined evaluation metric.\n\n2. **Feature Ranking:** Rank the remaining features based on their importance scores obtained from the trained model.\n\n3. **Feature Addition:** Add the feature with the highest importance score to the feature set.\n\n4. **Model Retraining:** Train the machine learning model again, this time using the new set of features.\n\n5. **Iteration:** Repeat steps 2-4 until a stopping criterion is met, such as reaching a desired number of features or observing a decline in model performance.\n\n6. **Final Model:** Train the machine learning model using the selected subset of features and evaluate its performance on a separate validation or test dataset.\n\nRFA shares similarities with both filter and wrapper methods:\n\n- **Filter-Like Aspect:** RFA ranks features based on their importance scores before adding them to the model, similar to the ranking process in filter methods.\n\n- **Wrapper-Like Aspect:** RFA iteratively trains and evaluates a machine learning model using different subsets of features, akin to the wrapper methods' iterative approach.\n\nRFA, like RFE, can be considered a hybrid method because it combines the benefits of a machine learning model's performance assessment with an iterative process for feature subset refinement. It seeks to find the most informative features for the model by iteratively adding them to the feature set.\n\nWhen using RFA, it's essential to consider the computational demands, the choice of the base model, and the potential for overfitting during the iterative process. RFA is particularly useful when interpretability and model performance are both critical aspects of the feature selection process.","metadata":{}},{"cell_type":"code","source":"from feature_engine.selection import RecursiveFeatureAddition\n\nalgorithm_start = time.time()\n\nGradient_Boosting = GradientBoostingClassifier(random_state = RANDOM_STATE)\n\nselector = RecursiveFeatureAddition(estimator = Gradient_Boosting, threshold = 0.0001)\n\nselector.fit(X, y)\n\nSelected_X = selector.transform(X)\n\nalgorithm_end = time.time()\n\nalgorithm_duration.append(algorithm_end - algorithm_start)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:04:57.177666Z","iopub.execute_input":"2023-08-20T19:04:57.178116Z","iopub.status.idle":"2023-08-20T19:05:01.982893Z","shell.execute_reply.started":"2023-08-20T19:04:57.178063Z","shell.execute_reply":"2023-08-20T19:05:01.981816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"differences(Selected_X, \"Recursive Feature Addition\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:01.984262Z","iopub.execute_input":"2023-08-20T19:05:01.984576Z","iopub.status.idle":"2023-08-20T19:05:02.004306Z","shell.execute_reply.started":"2023-08-20T19:05:01.984549Z","shell.execute_reply":"2023-08-20T19:05:02.002945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_results(Selected_X, y, TECHNIQUE_NAME = \"Recursive Feature Addition\", TECHNIQUE_TYPE = \"Hybrid\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:02.00583Z","iopub.execute_input":"2023-08-20T19:05:02.00632Z","iopub.status.idle":"2023-08-20T19:05:02.336531Z","shell.execute_reply.started":"2023-08-20T19:05:02.006274Z","shell.execute_reply":"2023-08-20T19:05:02.335376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Other methods to explore\n## Wrapper: Linear Regression\n\n\nThe \"linear regression\" wrapper method is a feature selection technique that involves using a linear regression model as the evaluation criterion to select relevant features for a regression task. This approach employs a linear regression model to evaluate different subsets of features and identifies the subset that leads to the best model performance.\n\nThe general process of using linear regression as a wrapper method for feature selection involves these steps:\n\n1. **Initialization:** Begin with an empty set of selected features.\n\n2. **Feature Evaluation:** For each feature not yet selected, train a linear regression model using the currently selected features along with the feature under consideration. Evaluate the model's performance using a predefined evaluation metric, such as mean squared error (MSE) or R-squared.\n\n3. **Feature Selection:** Select the feature that, when added to the currently selected features, results in the highest improvement in model performance according to the evaluation metric.\n\n4. **Iteration:** Repeat steps 2 and 3 until a stopping criterion is met. This criterion could be reaching a desired number of features or observing a decline in performance improvement.\n\n5. **Final Model:** Train the linear regression model using the selected features and evaluate its performance on a separate validation or test dataset.\n\nUsing linear regression as a wrapper method for feature selection offers certain advantages:\n\n- **Model Compatibility:** Linear regression is a natural choice for regression tasks, making it well-suited for evaluating feature relevance in a regression context.\n\n- **Interpretability:** Linear regression coefficients indicate the direction and strength of the relationship between each selected feature and the target variable, aiding in understanding their impact on predictions.\n\n- **Simplicity:** The linear regression model's simplicity allows for relatively quick iterations during the selection process.\n\nHowever, there are important considerations:\n\n- **Computational Demand:** Repeatedly training linear regression models can be computationally demanding, particularly with large datasets.\n\n- **Linear Assumption:** Linear regression assumes a linear relationship between features and the target variable. Non-linear relationships may not be accurately captured by this method.\n\n- **Overfitting:** Overfitting can occur if not managed properly. Cross-validation, regularization, and appropriate stopping criteria are essential.\n\nLinear regression-based wrapper methods are most effective when linear regression is a suitable model for the task, and when interpretability and model performance are key priorities. Combining wrapper methods with other techniques can provide a more comprehensive approach to feature selection, considering aspects such as non-linear relationships and computational efficiency.","metadata":{}},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"SUMMARY = create_summary()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:02.337786Z","iopub.execute_input":"2023-08-20T19:05:02.33814Z","iopub.status.idle":"2023-08-20T19:05:02.344682Z","shell.execute_reply.started":"2023-08-20T19:05:02.338109Z","shell.execute_reply":"2023-08-20T19:05:02.343457Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SUMMARY","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:02.346411Z","iopub.execute_input":"2023-08-20T19:05:02.346824Z","iopub.status.idle":"2023-08-20T19:05:02.397063Z","shell.execute_reply.started":"2023-08-20T19:05:02.346782Z","shell.execute_reply":"2023-08-20T19:05:02.395856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Computational demand\n\nTo evaluate the computational demands, I conducted measurements on two distinct parameters:\n\n**Training Time:** Training time refers to the amount of time it takes to train a machine learning model on a given dataset. During the training process, the model learns from the data to make predictions or classifications. The training time can vary significantly based on factors such as the complexity of the model, the size of the dataset, the hardware used (like GPUs or TPUs), and the optimization techniques employed.\n\nLonger training times might be required for more complex models (e.g., deep neural networks) or larger datasets. Techniques like mini-batch training, distributed training, and model parallelism can be used to speed up training time. The goal is to find a balance between training time and model performance, as longer training times might lead to better results but could also increase resource requirements.\n\n**Algorithm Runtime:** Algorithm runtime refers to the time it takes for the transformation (Feature Selection) technique to finish the entire process.The runtime of an algorithm can depend on the complexity of the transformation itself and the amount of computation required for each technique. For instance, simpler algorithm like filter methods might have very short runtimes, while more complex models like Exhaustive Selection might have longer runtimes due to the number of iterations and parameters.\n\nBoth training time and algorithm runtime are important considerations in machine learning, especially when deploying models in real-world applications. Balancing the trade-off between model complexity, training time, and runtime is crucial to ensure that the model can provide accurate predictions while being efficient in terms of resource usage.","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1, figsize = (10, 4), sharey = False)\n\nsns.barplot(data = SUMMARY, x = \"Type\", y = \"Time to transform\",\n            estimator = \"mean\", ax = ax1)\nax1.set_title(\"Time to transform\")\nsns.barplot(data = SUMMARY, x = \"Type\", y = \"Training time\",\n            estimator = \"mean\", ax = ax2)\nax2.set_title(\"Training time\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:02.398952Z","iopub.execute_input":"2023-08-20T19:05:02.39936Z","iopub.status.idle":"2023-08-20T19:05:03.252896Z","shell.execute_reply.started":"2023-08-20T19:05:02.399322Z","shell.execute_reply":"2023-08-20T19:05:03.25179Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SUMMARY.groupby(\"Type\").mean()[['Time to transform', \"Training time\"]].style.highlight_max(axis = 0,\n                                                                                           color = \"red\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:03.254074Z","iopub.execute_input":"2023-08-20T19:05:03.254385Z","iopub.status.idle":"2023-08-20T19:05:03.276656Z","shell.execute_reply.started":"2023-08-20T19:05:03.254359Z","shell.execute_reply":"2023-08-20T19:05:03.275659Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Wrapper methods for feature selection often involve lengthier data transformation compared to filter methods due to their iterative nature and reliance on model training and evaluation. This time discrepancy stems from various factors, including: the need for training and evaluating a machine learning model repeatedly for different feature subsets, which becomes computationally intensive for complex models or large datasets; the iterative process of assessing feature impacts on model performance introduces additional computation steps; some wrapper methods employ intricate models like ensembles or deep learning, necessitating more time for training and evaluation; cross-validation, commonly used to ensure robust performance assessment, increases computational workload; hyperparameter tuning in certain wrapper methods adds computational complexity; and techniques to prevent overfitting, such as cross-validation and regularization, contribute to computational demands.\n\nIn contrast, filter and hybrid methods are generally faster due to their independence from model specifics. They compute statistical metrics or scores directly from the data, without requiring multiple iterations of model training and evaluation. This characteristic makes filter and hybrid methods more efficient, especially for large datasets or limited computational resources. Despite wrapper methods' longer processing times, they offer accurate feature assessments by considering the model's behavior. The choice between filter and wrapper methods depends on factors like available resources, dataset size, desired model performance, and the trade-off between processing time and feature selection precision.","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols = 1, nrows = 2, figsize = (13, 10), sharex = False)\n\nsns.barplot(data = SUMMARY, y = \"Technique\", x = \"Time to transform\", ax = ax1)\nax1.axvline(SUMMARY['Time to transform'].mean(), color = 'r', label = \"Mean\")\nax1.set_title(\"Time to transform\")\nax1.legend(loc = \"lower right\")\nsns.barplot(data = SUMMARY, y = \"Technique\", x = \"Training time\", ax = ax2)\nax2.axvline(SUMMARY['Training time'].mean(), color = 'r', label = \"Mean\")\nax2.set_title(\"Training time\")\nax2.legend(loc = \"upper right\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:03.289323Z","iopub.execute_input":"2023-08-20T19:05:03.289726Z","iopub.status.idle":"2023-08-20T19:05:04.778893Z","shell.execute_reply.started":"2023-08-20T19:05:03.289693Z","shell.execute_reply":"2023-08-20T19:05:04.777908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It is noticeable that Exhaustive selection stands out for its capability to identify the most optimal feature subset, ensuring the highest potential model performance. However, its unparalleled accuracy comes at a cost: the method's intensive computational requirements make it less viable for datasets containing a substantial number of features. The primary culprit behind this computational burden lies in the combinatorial explosion of possibilities as features increase.\n\nFor instance, in a dataset with N features, exhaustively evaluating all 2^N possible combinations becomes exponentially demanding. The process involves repeatedly training and evaluating machine learning models for each subset, along with computing performance metrics to determine the best-performing set. This multiplies the model's training time and the overall computational load.\n\nGiven these constraints, more pragmatic alternatives gain prominence in real-world scenarios. Filter methods, wrapper methods, and hybrid approaches as we can see, they offer effective strategies for feature selection while mitigating the computational limitations of exhaustive selection. These approaches strike a balance between accuracy and efficiency, making them suitable for datasets characterized by a high feature count. \n\nFilter methods swiftly pre-assess feature relevance using statistical metrics or scores derived from the data itself. Wrapper methods, while more time-consuming than filters, iteratively fine-tune the feature subset based on the model's performance. Hybrid approaches, taking cues from both filter and wrapper methods, harness the strengths of each to achieve more practical solutions.","metadata":{}},{"cell_type":"markdown","source":"## Computational trade-off","metadata":{}},{"cell_type":"code","source":"temp = SUMMARY[['Technique', 'Type', 'Time to transform', 'Training time']]\n\nscaler = StandardScaler()\ntemp[['Time to transform', 'Training time']] = scaler.fit_transform(temp[['Time to transform', 'Training time']])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:04.780107Z","iopub.execute_input":"2023-08-20T19:05:04.781134Z","iopub.status.idle":"2023-08-20T19:05:04.79273Z","shell.execute_reply.started":"2023-08-20T19:05:04.781101Z","shell.execute_reply":"2023-08-20T19:05:04.791442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1, figsize = (10, 5))\n\nsns.scatterplot(data = temp, x = \"Time to transform\",\n                y = \"Training time\", hue = \"Type\", s = 100, ax = ax1, edgecolor = 'k')\nax1.grid(False)\nax1.set_ylim(-4, 4)\nax1.set_xlim(-4, 4)\nax1.legend(loc = \"lower left\")\nax1.axhline(0, color = 'k', alpha = 0.4)\nax1.axvline(0, color = 'k', alpha = 0.4)\nax1.set_title(\"Sampling Algorithm Type\")\n\nsns.scatterplot(data = temp, x = \"Time to transform\",\n                y = \"Training time\", hue = \"Technique\", s = 100, ax = ax2, edgecolor = 'k')\nax2.grid(False)\nax2.set_ylim(-4, 4)\nax2.set_xlim(-4, 4)\nax2.legend(loc = 'center left',  bbox_to_anchor = (1, 0.5))\nax2.axhline(0, color = \"k\", alpha = 0.4)\nax2.axvline(0, color = \"k\", alpha = 0.4)\nax2.set_title(\"Sampling Algorithm Technique\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:04.794411Z","iopub.execute_input":"2023-08-20T19:05:04.794828Z","iopub.status.idle":"2023-08-20T19:05:06.733811Z","shell.execute_reply.started":"2023-08-20T19:05:04.794791Z","shell.execute_reply":"2023-08-20T19:05:06.732709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SUMMARY.groupby([\"Type\", \"Technique\"])[[\"Time to transform\", \"Training time\"]].mean().style.highlight_max(axis=0, color = \"red\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:06.735219Z","iopub.execute_input":"2023-08-20T19:05:06.736341Z","iopub.status.idle":"2023-08-20T19:05:06.756922Z","shell.execute_reply.started":"2023-08-20T19:05:06.736309Z","shell.execute_reply":"2023-08-20T19:05:06.755703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Technique and model performance\nIn the course of evaluating the effectiveness of various techniques employed within this notebook, I have compiled a set of metrics that remain unbiased by features selection.\nThe metrics we are gonna be reviewing are the following:\n\n**AUC Score (Area Under the ROC Curve):** The AUC (Area Under the Receiver Operating Characteristic Curve) score is a widely used metric for evaluating the performance of binary classification models. The ROC curve is created by plotting the True Positive Rate (Recall) against the False Positive Rate at various classification thresholds. The AUC score quantifies the area under this curve, ranging from 0 to 1. A higher AUC score indicates better discriminative ability of the model in distinguishing between the two classes. An AUC score of 0.5 suggests random guessing, while a score closer to 1 signifies excellent model performance.\n\n**Accuracy:** Accuracy is a basic metric that measures the ratio of correctly predicted instances to the total number of instances in a dataset. It provides a general overview of the model's overall correctness. However, accuracy might not be suitable for imbalanced datasets where one class significantly outweighs the other. In such cases, high accuracy can be achieved by merely predicting the majority class, even if the minority class predictions are poor.\n\n**Precision:** Precision, also known as Positive Predictive Value, gauges the proportion of true positive predictions out of all positive predictions made by the model. It's a measure of how well the model avoids falsely labeling instances as positive. A high precision indicates that when the model predicts a positive outcome, it is likely to be correct.\n\n**Recall (Sensitivity):** Recall, also called Sensitivity or True Positive Rate, calculates the proportion of true positive predictions out of all actual positive instances in the dataset. It measures the model's ability to correctly identify positive cases. High recall implies that the model effectively captures most of the true positives, although it might also generate false positives.\n\n**F1 Score:** The F1 Score is the harmonic mean of precision and recall. It offers a balanced perspective on a model's performance, especially when the class distribution is imbalanced. It takes both false positives and false negatives into account and is thus a useful metric when aiming for a balance between precision and recall.\n\nThese metrics collectively provide insights into different aspects of model performance, allowing practitioners to make informed decisions based on the specific goals and challenges of their classification tasks.","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(ncols = 5, nrows = 1, figsize = (15, 4), sharey = False)\n\naxes = [ax1, ax2, ax3, ax4, ax5]\n\nfor ax, feature in enumerate([\"AUC SCORE\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]):\n    \n    sns.barplot(data = SUMMARY, x = \"Type\", y = feature, ax = axes[ax], edgecolor = \"k\")\n    axes[ax].set_xticklabels(SUMMARY[\"Type\"].unique(), rotation = 90)\n    axes[ax].set_ylim(bottom = SUMMARY[feature].min() - 0.01)\n    axes[ax].set_title(f\"{feature}\")\n    axes[ax].set_ylabel(\"\")\nplt.tight_layout()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:06.758261Z","iopub.execute_input":"2023-08-20T19:05:06.75916Z","iopub.status.idle":"2023-08-20T19:05:08.845619Z","shell.execute_reply.started":"2023-08-20T19:05:06.759125Z","shell.execute_reply":"2023-08-20T19:05:08.844515Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Overall score","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler \n\ntemp = SUMMARY[['Technique', 'Type', 'AUC SCORE', 'Accuracy', 'Precision', 'Recall', \"F1 Score\"]]\n\nscaler = MinMaxScaler()\ntemp[['AUC SCORE', 'Accuracy', 'Precision', 'Recall', \"F1 Score\"]] = scaler.fit_transform(temp[['AUC SCORE', 'Accuracy', 'Precision', 'Recall', \"F1 Score\"]])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:08.847024Z","iopub.execute_input":"2023-08-20T19:05:08.847352Z","iopub.status.idle":"2023-08-20T19:05:08.861322Z","shell.execute_reply.started":"2023-08-20T19:05:08.847324Z","shell.execute_reply":"2023-08-20T19:05:08.860532Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp[\"Overall score\"] = temp[['AUC SCORE', 'Accuracy', 'Precision', 'Recall', \"F1 Score\"]].sum(axis = 1)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:08.862696Z","iopub.execute_input":"2023-08-20T19:05:08.863605Z","iopub.status.idle":"2023-08-20T19:05:08.87771Z","shell.execute_reply.started":"2023-08-20T19:05:08.863558Z","shell.execute_reply":"2023-08-20T19:05:08.876561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (8, 4))\nsns.barplot(data = temp, y = \"Type\", x = \"Overall score\")\nplt.title(\"Sum of scaled metrics\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:08.879333Z","iopub.execute_input":"2023-08-20T19:05:08.879742Z","iopub.status.idle":"2023-08-20T19:05:09.398383Z","shell.execute_reply.started":"2023-08-20T19:05:08.879712Z","shell.execute_reply":"2023-08-20T19:05:09.397575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp.groupby(\"Type\").mean().sort_values(by = \"Overall score\", ascending = False).style.highlight_max(axis = 0, color = \"lightgreen\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:09.399776Z","iopub.execute_input":"2023-08-20T19:05:09.400319Z","iopub.status.idle":"2023-08-20T19:05:09.418669Z","shell.execute_reply.started":"2023-08-20T19:05:09.400287Z","shell.execute_reply":"2023-08-20T19:05:09.417609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(ncols = 1, nrows = 5, figsize = (10, 17), sharey = False)\n\naxes = [ax1, ax2, ax3, ax4, ax5]\n\nfor ax, feature in enumerate([\"AUC SCORE\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]):\n    \n    sns.barplot(data = SUMMARY, y = \"Technique\", x = feature, ax = axes[ax], edgecolor = \"k\")\n    axes[ax].set_xlim(left = SUMMARY[feature].min() - 0.01, right = SUMMARY[feature].max() + 0.01)\n    axes[ax].set_title(f\"{feature}\")\n    axes[ax].set_xlabel(\"\")\n    axes[ax].set_ylabel(\"\")\n    \nplt.tight_layout()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:09.420203Z","iopub.execute_input":"2023-08-20T19:05:09.420653Z","iopub.status.idle":"2023-08-20T19:05:12.429942Z","shell.execute_reply.started":"2023-08-20T19:05:09.420613Z","shell.execute_reply":"2023-08-20T19:05:12.428845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (8, 4))\nsns.barplot(data = temp, y = \"Technique\", x = \"Overall score\", edgecolor = 'k')\nplt.xlim(left = 2.3, right = 5.02)\nplt.title(\"Sum of scaled metrics\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:12.431327Z","iopub.execute_input":"2023-08-20T19:05:12.431752Z","iopub.status.idle":"2023-08-20T19:05:13.0668Z","shell.execute_reply.started":"2023-08-20T19:05:12.431723Z","shell.execute_reply":"2023-08-20T19:05:13.065486Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"temp.groupby(\"Technique\").mean().sort_values(by = \"Overall score\", ascending = False).style.highlight_max(axis = 0, color = \"lightgreen\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:13.068355Z","iopub.execute_input":"2023-08-20T19:05:13.068748Z","iopub.status.idle":"2023-08-20T19:05:13.093429Z","shell.execute_reply.started":"2023-08-20T19:05:13.068715Z","shell.execute_reply":"2023-08-20T19:05:13.092373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (9, 7))\nsns.scatterplot(data = temp, x = \"AUC SCORE\"\n                , y = \"Technique\", s = 200, edgecolor = 'k', label = \"AUC SCORE\", )\n\nsns.scatterplot(data = temp, x = \"Accuracy\"\n                , y = \"Technique\", s = 200, edgecolor = 'k', label = \"Accuracy\")\n\nsns.scatterplot(data = temp, x = \"Precision\"\n                , y = \"Technique\", s = 200, edgecolor = 'k', label = \"Precision\")\n\nsns.scatterplot(data = temp, x = \"Recall\"\n                , y = \"Technique\", s = 200, edgecolor = 'k', label = \"Recall\")\n\nsns.scatterplot(data = temp, x = \"F1 Score\"\n                , y = \"Technique\", s = 200, edgecolor = 'k', label = \"F1 Score\")\n\nplt.legend(loc = 'center left',  bbox_to_anchor = (1, 0.5))\nplt.xlabel(\"Score 0 to 1\")\nplt.title(\"All metrics vs. Technique\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:13.094871Z","iopub.execute_input":"2023-08-20T19:05:13.095919Z","iopub.status.idle":"2023-08-20T19:05:14.103783Z","shell.execute_reply.started":"2023-08-20T19:05:13.095881Z","shell.execute_reply":"2023-08-20T19:05:14.102667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (7, 5))\n\nsns.scatterplot(data = temp, x = \"AUC SCORE\"\n                , y = \"Type\", s = 200, edgecolor = 'k', label = \"AUC SCORE\")\n\nsns.scatterplot(data = temp, x = \"Accuracy\"\n                , y = \"Type\", s = 200, edgecolor = 'k', label = \"Accuracy\")\n\nsns.scatterplot(data = temp, x = \"Precision\"\n                , y = \"Type\", s = 200, edgecolor = 'k', label = \"Precision\")\n\nsns.scatterplot(data = temp, x = \"Recall\"\n                , y = \"Type\", s = 200, edgecolor = 'k', label = \"Recall\")\n\nsns.scatterplot(data = temp, x = \"F1 Score\"\n                , y = \"Type\", s = 200, edgecolor = 'k', label = \"F1 Score\")\n\nplt.xlabel(\"Score 0 to 1\")\nplt.legend(loc = 'center left',  bbox_to_anchor = (1, 0.5))\nplt.title(\"All metrics vs. Type\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:14.105255Z","iopub.execute_input":"2023-08-20T19:05:14.105677Z","iopub.status.idle":"2023-08-20T19:05:14.76602Z","shell.execute_reply.started":"2023-08-20T19:05:14.105639Z","shell.execute_reply":"2023-08-20T19:05:14.76497Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Top Features dropped and kept","metadata":{}},{"cell_type":"code","source":"DROPPED = list()\nKEPT = list()\n\nfor list_droped in SUMMARY['Features dropped'].to_list():\n    DROPPED.extend(list_droped)\n    \nfor list_kept in SUMMARY['Features kept'].to_list():\n    KEPT.extend(list_kept)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:14.767697Z","iopub.execute_input":"2023-08-20T19:05:14.768216Z","iopub.status.idle":"2023-08-20T19:05:14.774896Z","shell.execute_reply.started":"2023-08-20T19:05:14.76817Z","shell.execute_reply":"2023-08-20T19:05:14.773978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\ndropped_names, dropped_values = list(Counter(DROPPED).keys()), list(Counter(DROPPED).values())\n\nkept_names, kept_values = list(Counter(KEPT).keys()), list(Counter(KEPT).values())\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1, figsize = (10, 4))\n\nax1.bar(range(len(Counter(DROPPED))), dropped_values, tick_label = dropped_names)\nax1.set_xticklabels(dropped_names, rotation = 90)\nax1.set_title(\"Features dropped\")\nax1.set_ylabel(\"Times dropped\")\nax2.bar(range(len(Counter(KEPT))), kept_values, tick_label = kept_names)\nax2.set_xticklabels(kept_names, rotation = 90)\nax2.set_title(\"Features kept\")\nax2.set_ylabel(\"Times kept\");","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:14.776119Z","iopub.execute_input":"2023-08-20T19:05:14.777194Z","iopub.status.idle":"2023-08-20T19:05:15.611603Z","shell.execute_reply.started":"2023-08-20T19:05:14.77715Z","shell.execute_reply":"2023-08-20T19:05:15.61046Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = pd.DataFrame([dict(Counter(DROPPED)), dict(Counter(KEPT))],\n             index = [\"Dropped\", \"Kept\"]).fillna(0).T\nFEATURES","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:15.613311Z","iopub.execute_input":"2023-08-20T19:05:15.61405Z","iopub.status.idle":"2023-08-20T19:05:15.629724Z","shell.execute_reply.started":"2023-08-20T19:05:15.614016Z","shell.execute_reply":"2023-08-20T19:05:15.628514Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Most dropped or worst featurees\nFEATURES[\"Dropped\"].sort_values(ascending = False).to_frame().head(3)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:15.63132Z","iopub.execute_input":"2023-08-20T19:05:15.631867Z","iopub.status.idle":"2023-08-20T19:05:15.646041Z","shell.execute_reply.started":"2023-08-20T19:05:15.631828Z","shell.execute_reply":"2023-08-20T19:05:15.645067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Most kept or best featurees\nFEATURES[\"Kept\"].sort_values(ascending = False).to_frame().head(3)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:15.647355Z","iopub.execute_input":"2023-08-20T19:05:15.647938Z","iopub.status.idle":"2023-08-20T19:05:15.658431Z","shell.execute_reply.started":"2023-08-20T19:05:15.647907Z","shell.execute_reply":"2023-08-20T19:05:15.657324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Average dropping by technique and type","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols = 1, nrows = 2, figsize = (5, 8), sharex = True)\n\nsns.barplot(data = SUMMARY, y = \"Technique\", x = \"Total dropped\", ax = ax1, edgecolor = 'k')\nax1.axvline(SUMMARY[\"Total dropped\"].mean())\n\n\nsns.barplot(data = SUMMARY, y = \"Type\", x = \"Total dropped\", ax = ax2, edgecolor = 'k')\nax2.axvline(SUMMARY[\"Total dropped\"].mean());","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-08-20T19:05:15.660025Z","iopub.execute_input":"2023-08-20T19:05:15.660865Z","iopub.status.idle":"2023-08-20T19:05:16.648005Z","shell.execute_reply.started":"2023-08-20T19:05:15.660834Z","shell.execute_reply":"2023-08-20T19:05:16.646843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Thoughts\n\nWhile feature selection undeniably plays a pivotal role in enhancing the performance and interpretability of machine learning models, it's essential to recognize that it's just one piece of the broader puzzle. The journey toward building robust and impactful machine learning solutions involves a multifaceted approach that extends beyond feature selection.\n\n**Model Architecture and Complexity:** Choosing an appropriate model architecture tailored to the problem at hand is equally vital. The model's complexity should be aligned with the complexity of the data and the underlying relationships. Striking the right balance can prevent overfitting or underfitting, which can greatly affect a model's generalization capabilities.\n\n**Data Preprocessing:** Feature engineering, data cleaning, normalization, and handling missing values all contribute to the overall data preprocessing pipeline. Neglecting these steps can lead to suboptimal model performance and erroneous insights. A meticulous data preprocessing process enhances the quality of the features available for selection.\n\n**Hyperparameter Tuning:** The model's hyperparameters profoundly impact its performance. Properly tuning hyperparameters can transform a mediocre model into a highly accurate one. Ignoring this step risks leaving performance gains on the table.\n\n**Domain Knowledge:** Incorporating domain expertise can significantly enrich the feature selection process. A thorough understanding of the problem domain can guide the selection of features that are not only relevant but also align with the underlying mechanics of the problem.\n\n**Validation and Testing:** Robust validation and testing procedures are critical to ensure that the selected features and the trained model generalize well to unseen data. This guards against over-optimization and helps evaluate a model's real-world performance.\n\n**Interpretability:** While optimizing model performance is paramount, interpretability should not be overlooked. Understanding the relationships between features and predictions is crucial for building trust in the model's decisions.\n\nIn conclusion, feature selection is undoubtedly a cornerstone of effective machine learning, but it exists within a broader ecosystem of considerations. Embracing a holistic approach that encompasses model architecture, data preprocessing, hyperparameter tuning, domain knowledge, validation, testing, and interpretability ensures that the end result is not only accurate but also reliable, insightful, and capable of driving informed decisions.\n","metadata":{}},{"cell_type":"markdown","source":"# References\n\n- [Feature-Engine](https://feature-engine.trainindata.com/en/latest/)\n- [MLxtend](https://rasbt.github.io/mlxtend/)\n- [Scikit-learn](https://scikit-learn.org/stable/)\n- [Feature Selection for Machine Learning](https://www.trainindata.com/p/feature-selection-for-machine-learning)","metadata":{}}]}